"""
Data Analyzer Module
===================

Module pour l'analyse des donn√©es extraites de Stack Overflow.
Utilise des techniques de NLP et d'analyse statistique pour identifier les tendances.

Classes:
    DataAnalyzer: Analyseur principal des donn√©es
    TrendAnalyzer: Analyseur de tendances sp√©cialis√©
    NLPProcessor: Processeur de traitement du langage naturel
"""

import asyncio
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Tuple, Optional
from collections import Counter, defaultdict
import json
import re

import pandas as pd
import numpy as np
from textblob import TextBlob
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import LatentDirichletAllocation

from .database import DatabaseManager


class NLPProcessor:
    """Processeur de traitement du langage naturel pour l'analyse de texte."""
    
    def __init__(self):
        """Initialise le processeur NLP."""
        self.logger = logging.getLogger(__name__)
        self.lemmatizer = WordNetLemmatizer()
        self.stop_words = set(stopwords.words('english'))
        
        # T√©l√©charger les ressources NLTK n√©cessaires
        self._download_nltk_resources()
    
    def _download_nltk_resources(self):
        """T√©l√©charge les ressources NLTK n√©cessaires."""
        resources = ['punkt', 'stopwords', 'wordnet', 'vader_lexicon']
        for resource in resources:
            try:
                nltk.data.find(f'tokenizers/{resource}')
            except LookupError:
                nltk.download(resource, quiet=True)
    
    def preprocess_text(self, text: str) -> str:
        """
        Pr√©traite un texte pour l'analyse NLP.
        
        Args:
            text: Texte √† pr√©traiter
            
        Returns:
            Texte pr√©trait√©
        """
        if not text:
            return ""
        
        # Nettoyage de base
        text = text.lower()
        text = re.sub(r'<[^>]+>', '', text)  # Supprimer HTML
        text = re.sub(r'http\S+|www\S+', '', text)  # Supprimer URLs
        text = re.sub(r'[^a-zA-Z\s]', '', text)  # Garder seulement lettres et espaces
        
        # Tokenisation et lemmatisation
        tokens = word_tokenize(text)
        tokens = [self.lemmatizer.lemmatize(token) for token in tokens 
                 if token not in self.stop_words and len(token) > 2]
        
        return ' '.join(tokens)
    
    def extract_keywords(self, texts: List[str], max_features: int = 100) -> List[Tuple[str, float]]:
        """
        Extrait les mots-cl√©s les plus importants d'une collection de textes.
        
        Args:
            texts: Liste des textes √† analyser
            max_features: Nombre maximum de mots-cl√©s √† extraire
            
        Returns:
            Liste des mots-cl√©s avec leurs scores TF-IDF
        """
        if not texts:
            return []
        
        # Pr√©traitement
        processed_texts = [self.preprocess_text(text) for text in texts]
        processed_texts = [text for text in processed_texts if text.strip()]
        
        if not processed_texts:
            return []
        
        # Adaptation des param√®tres selon le nombre de documents
        num_docs = len(processed_texts)
        
        # min_df adaptatif : minimum 1, mais pas plus de 10% des documents
        adaptive_min_df = max(1, min(2, int(num_docs * 0.1)))
        
        # max_df adaptatif : entre 0.7 et 0.95 selon le nombre de documents
        if num_docs <= 5:
            adaptive_max_df = 0.95
        elif num_docs <= 10:
            adaptive_max_df = 0.9
        else:
            adaptive_max_df = 0.8
        
        # TF-IDF avec param√®tres adaptatifs
        vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(1, 2),
            min_df=adaptive_min_df,
            max_df=adaptive_max_df
        )
        
        try:
            tfidf_matrix = vectorizer.fit_transform(processed_texts)
            feature_names = vectorizer.get_feature_names_out()
            
            # Calcul des scores moyens
            mean_scores = np.mean(tfidf_matrix.toarray(), axis=0)
            
            # Cr√©ation de la liste des mots-cl√©s avec scores
            keywords = list(zip(feature_names, mean_scores))
            keywords.sort(key=lambda x: x[1], reverse=True)
            
            return keywords
            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'extraction de mots-cl√©s: {e}")
            return []
    
    def analyze_sentiment(self, texts: List[str]) -> Dict[str, Any]:
        """
        Analyse le sentiment d'une collection de textes.
        
        Args:
            texts: Liste des textes √† analyser
            
        Returns:
            Statistiques de sentiment
        """
        if not texts:
            return {"positive": 0, "negative": 0, "neutral": 0, "average": 0}
        
        sentiments = []
        
        for text in texts:
            if text and text.strip():
                blob = TextBlob(text)
                polarity = blob.sentiment.polarity
                sentiments.append(polarity)
        
        if not sentiments:
            return {"positive": 0, "negative": 0, "neutral": 0, "average": 0}
        
        # Classification des sentiments
        positive = sum(1 for s in sentiments if s > 0.1)
        negative = sum(1 for s in sentiments if s < -0.1)
        neutral = len(sentiments) - positive - negative
        
        return {
            "positive": positive,
            "negative": negative,
            "neutral": neutral,
            "average": np.mean(sentiments),
            "total": len(sentiments)
        }


class TrendAnalyzer:
    """Analyseur de tendances pour les donn√©es Stack Overflow."""
    
    def __init__(self):
        """Initialise l'analyseur de tendances."""
        self.logger = logging.getLogger(__name__)
    
    def analyze_tag_trends(self, questions: List[Dict]) -> Dict[str, Any]:
        """
        Analyse les tendances des tags.
        
        Args:
            questions: Liste des questions
            
        Returns:
            Analyse des tendances des tags
        """
        if not questions:
            return {}
        
        # Extraction des tags avec dates
        tag_timeline = defaultdict(list)
        
        for question in questions:
            date = question.get('publication_date')
            tags = question.get('tags', [])
            
            if isinstance(date, str):
                try:
                    date = datetime.fromisoformat(date.replace('Z', '+00:00'))
                except:
                    continue
            
            for tag in tags:
                tag_timeline[tag].append(date)
        
        # Analyse par p√©riode
        trends = {}
        current_date = datetime.now()
        periods = {
            'last_week': current_date - timedelta(days=7),
            'last_month': current_date - timedelta(days=30),
            'last_quarter': current_date - timedelta(days=90),
            'last_year': current_date - timedelta(days=365)
        }
        
        for tag, dates in tag_timeline.items():
            if len(dates) < 1:  # Analyser tous les tags qui apparaissent au moins une fois
                continue
            
            tag_trend = {'tag': tag, 'total_questions': len(dates)}
            
            for period_name, period_start in periods.items():
                count = sum(1 for date in dates if date >= period_start)
                tag_trend[period_name] = count
            
            # Calcul de la tendance (croissance r√©cente vs ancienne)
            recent_count = tag_trend['last_month']
            older_count = len([d for d in dates 
                              if current_date - timedelta(days=60) <= d < current_date - timedelta(days=30)])
            
            if older_count > 0:
                growth_rate = (recent_count - older_count) / older_count * 100
            elif recent_count > 0:
                # Si pas de donn√©es anciennes mais des r√©centes, afficher comme "nouveau" plut√¥t que 100%
                growth_rate = 0  # Marquer comme nouveau tag plut√¥t que croissance infinie
            else:
                growth_rate = 0
            
            tag_trend['growth_rate'] = growth_rate
            trends[tag] = tag_trend
        
        # Tri par popularit√© et croissance
        trending_tags = sorted(
            trends.values(),
            key=lambda x: (x['growth_rate'], x['last_month']),
            reverse=True
        )
        
        return {
            'trending_tags': trending_tags[:20],
            'top_tags': sorted(trends.values(), key=lambda x: x['total_questions'], reverse=True)[:20],
            'analysis_date': current_date.isoformat()
        }
    
    def analyze_temporal_patterns(self, questions: List[Dict]) -> Dict[str, Any]:
        """
        Analyse les patterns temporels des questions.
        
        Args:
            questions: Liste des questions
            
        Returns:
            Analyse des patterns temporels
        """
        if not questions:
            return {}
        
        # Conversion en DataFrame pour faciliter l'analyse
        df_data = []
        for question in questions:
            date = question.get('publication_date')
            if isinstance(date, str):
                try:
                    date = datetime.fromisoformat(date.replace('Z', '+00:00'))
                except:
                    continue
            
            if date:
                df_data.append({
                    'date': date,
                    'hour': date.hour,
                    'day_of_week': date.weekday(),
                    'month': date.month,
                    'votes': question.get('vote_count', 0),
                    'views': question.get('view_count', 0),
                    'answers': question.get('answer_count', 0)
                })
        
        if not df_data:
            return {}
        
        df = pd.DataFrame(df_data)
        
        # Analyse par heure
        hourly_stats = df.groupby('hour').agg({
            'votes': ['count', 'mean'],
            'views': 'mean',
            'answers': 'mean'
        }).round(2)
        
        # Analyse par jour de la semaine
        daily_stats = df.groupby('day_of_week').agg({
            'votes': ['count', 'mean'],
            'views': 'mean',
            'answers': 'mean'
        }).round(2)
        
        # Analyse par mois
        monthly_stats = df.groupby('month').agg({
            'votes': ['count', 'mean'],
            'views': 'mean',
            'answers': 'mean'
        }).round(2)
        
        return {
            'hourly_patterns': hourly_stats.to_dict(),
            'daily_patterns': daily_stats.to_dict(),
            'monthly_patterns': monthly_stats.to_dict(),
            'peak_hour': int(df.groupby('hour').size().idxmax()),
            'peak_day': int(df.groupby('day_of_week').size().idxmax()),
            'total_questions_analyzed': len(df)
        }


class DataAnalyzer:
    """
    Analyseur principal des donn√©es Stack Overflow.
    
    Coordonne les diff√©rents types d'analyses et g√©n√®re des rapports complets.
    """
    
    def __init__(self, database_manager: DatabaseManager):
        """
        Initialise l'analyseur de donn√©es.
        
        Args:
            database_manager: Gestionnaire de base de donn√©es
        """
        self.db_manager = database_manager
        self.logger = logging.getLogger(__name__)
        self.nlp_processor = NLPProcessor()
        self.trend_analyzer = TrendAnalyzer()
        self.execution_metadata = {}
    
    def set_execution_metadata(self, scraping_info: Dict[str, Any]) -> None:
        """
        Configure les m√©tadonn√©es d'ex√©cution du scraping.
        
        Args:
            scraping_info: Informations sur l'ex√©cution du scraping
        """
        self.execution_metadata = scraping_info
    
    async def analyze_trends(self) -> Dict[str, Any]:
        """
        Effectue une analyse compl√®te des tendances.
        
        Returns:
            R√©sultats d'analyse complets
        """
        self.logger.info("[ANALYZE] D√©but de l'analyse des tendances")
        start_time = datetime.now()
        
        try:
            # R√©cup√©ration des donn√©es
            self.logger.info("Questions extraites: R√©cup√©ration des donn√©es depuis la base...")
            questions = await self.db_manager.get_questions(limit=5000)
            
            if not questions:
                self.logger.warning("‚ö†Ô∏è Aucune question trouv√©e pour l'analyse")
                return {"error": "Aucune donn√©e disponible"}
            
            self.logger.info(f"[OK] {len(questions)} questions r√©cup√©r√©es pour l'analyse")
            
            # Analyses principales
            results = {
                'execution_info': self.execution_metadata,
                'analysis_metadata': {
                    'analysis_date': datetime.now().isoformat(),
                    'total_questions': len(questions),
                    'date_range': self._get_date_range(questions)
                }
            }
            
            # 1. Analyse des tags et tendances
            self.logger.info("[TAGS]  √âtape 1/5: Analyse des tendances des tags...")
            results['tag_trends'] = self.trend_analyzer.analyze_tag_trends(questions)
            self.logger.info("[OK] Analyse des tags termin√©e")
            
            # 2. Analyse temporelle
            self.logger.info("[TIME] √âtape 2/5: Analyse des patterns temporels...")
            results['temporal_patterns'] = self.trend_analyzer.analyze_temporal_patterns(questions)
            self.logger.info("[OK] Analyse temporelle termin√©e")
            
            # 3. Analyse NLP des titres et r√©sum√©s
            self.logger.info("[NLP] √âtape 3/5: Analyse NLP des contenus...")
            results['content_analysis'] = await self._analyze_content(questions)
            self.logger.info("[OK] Analyse NLP termin√©e")
            
            # 4. Analyse des auteurs
            self.logger.info("[AUTHORS] √âtape 4/5: Analyse des auteurs...")
            results['author_analysis'] = await self._analyze_authors()
            self.logger.info("[OK] Analyse des auteurs termin√©e")
            
            # 5. Statistiques g√©n√©rales
            self.logger.info("[STATS] √âtape 5/5: Calcul des statistiques g√©n√©rales...")
            results['general_stats'] = await self._calculate_general_stats(questions)
            self.logger.info("[OK] Statistiques g√©n√©rales calcul√©es")
            
            # Temps d'ex√©cution
            end_time = datetime.now()
            results['analysis_metadata']['duration'] = (end_time - start_time).total_seconds()
            
            self.logger.info("üéâ ANALYSE COMPL√àTE TERMIN√âE!")
            self.logger.info(f"Temps total:  Dur√©e totale: {results['analysis_metadata']['duration']:.2f} secondes")
            self.logger.info(f"Questions extraites: Questions analys√©es: {len(questions)}")
            
            return results
            
        except Exception as e:
            self.logger.error(f"Erreur lors de l'analyse: {e}")
            raise
    
    def _get_date_range(self, questions: List[Dict]) -> Dict[str, str]:
        """Calcule la plage de dates des questions."""
        dates = []
        for question in questions:
            date = question.get('publication_date')
            if isinstance(date, str):
                try:
                    date = datetime.fromisoformat(date.replace('Z', '+00:00'))
                    dates.append(date)
                except:
                    continue
            elif isinstance(date, datetime):
                dates.append(date)
        
        if dates:
            return {
                'start': min(dates).isoformat(),
                'end': max(dates).isoformat()
            }
        return {'start': None, 'end': None}
    
    async def _analyze_content(self, questions: List[Dict]) -> Dict[str, Any]:
        """Analyse le contenu textuel des questions."""
        # Extraction des textes
        titles = [q.get('title', '') for q in questions]
        summaries = [q.get('summary', '') for q in questions]
        
        # Mots-cl√©s des titres
        title_keywords = self.nlp_processor.extract_keywords(titles, max_features=50)
        
        # Mots-cl√©s des r√©sum√©s
        summary_keywords = self.nlp_processor.extract_keywords(summaries, max_features=50)
        
        # Analyse de sentiment
        title_sentiment = self.nlp_processor.analyze_sentiment(titles)
        summary_sentiment = self.nlp_processor.analyze_sentiment(summaries)
        
        return {
            'title_keywords': title_keywords[:20],
            'summary_keywords': summary_keywords[:20],
            'title_sentiment': title_sentiment,
            'summary_sentiment': summary_sentiment,
            'average_title_length': np.mean([len(title) for title in titles if title]),
            'average_summary_length': np.mean([len(summary) for summary in summaries if summary])
        }
    
    async def _analyze_authors(self) -> Dict[str, Any]:
        """Analyse les donn√©es des auteurs."""
        authors = await self.db_manager.get_top_authors(limit=100)
        
        if not authors:
            return {}
        
        # Statistiques de r√©putation
        reputations = [author.get('reputation', 0) for author in authors]
        question_counts = [author.get('question_count', 0) for author in authors]
        
        return {
            'top_authors': authors[:10],
            'reputation_stats': {
                'mean': np.mean(reputations),
                'median': np.median(reputations),
                'max': max(reputations),
                'min': min(reputations)
            },
            'activity_stats': {
                'mean_questions_per_author': np.mean(question_counts),
                'median_questions_per_author': np.median(question_counts),
                'most_active_author': max(authors, key=lambda x: x.get('question_count', 0))
            },
            'total_authors': len(authors)
        }
    
    async def _calculate_general_stats(self, questions: List[Dict]) -> Dict[str, Any]:
        """Calcule les statistiques g√©n√©rales."""
        if not questions:
            return {}
        
        # Statistiques num√©riques
        votes = [q.get('vote_count', 0) for q in questions]
        views = [q.get('view_count', 0) for q in questions]
        answers = [q.get('answer_count', 0) for q in questions]
        
        # Statistiques des tags
        all_tags = []
        for q in questions:
            all_tags.extend(q.get('tags', []))
        
        tag_counter = Counter(all_tags)
        
        return {
            'vote_stats': {
                'mean': np.mean(votes),
                'median': np.median(votes),
                'max': max(votes),
                'std': np.std(votes)
            },
            'view_stats': {
                'mean': np.mean(views),
                'median': np.median(views),
                'max': max(views),
                'std': np.std(views)
            },
            'answer_stats': {
                'mean': np.mean(answers),
                'median': np.median(answers),
                'max': max(answers),
                'unanswered_rate': sum(1 for a in answers if a == 0) / len(answers) * 100
            },
            'tag_stats': {
                'total_unique_tags': len(tag_counter),
                'most_common_tags': tag_counter.most_common(20),
                'average_tags_per_question': len(all_tags) / len(questions)
            }
        }
    
    def _clean_for_mongodb(self, data):
        """
        Nettoie les donn√©es pour qu'elles soient compatibles avec MongoDB.
        Convertit les tuples en cha√Ænes, les types numpy en types Python natifs, etc.
        
        Args:
            data: Donn√©es √† nettoyer
            
        Returns:
            Donn√©es nettoy√©es compatibles avec MongoDB
        """
        if isinstance(data, dict):
            cleaned = {}
            for key, value in data.items():
                # Convertir les cl√©s tuples en cha√Ænes
                if isinstance(key, tuple):
                    key_str = "_".join(str(k) for k in key)
                else:
                    key_str = str(key)
                
                # Nettoyer r√©cursivement la valeur
                cleaned[key_str] = self._clean_for_mongodb(value)
            return cleaned
        
        elif isinstance(data, list):
            return [self._clean_for_mongodb(item) for item in data]
        
        elif isinstance(data, tuple):
            return list(data)
        
        elif isinstance(data, np.integer):
            return int(data)
        
        elif isinstance(data, np.floating):
            if np.isnan(data):
                return None
            return float(data)
        
        elif isinstance(data, np.ndarray):
            return data.tolist()
        
        elif hasattr(data, 'item'):  # numpy scalars
            return data.item()
        
        else:
            return data

    async def save_results(self, results: Dict[str, Any]) -> None:
        """
        Sauvegarde les r√©sultats d'analyse.
        
        Args:
            results: R√©sultats √† sauvegarder
        """
        # Nettoyer les donn√©es pour MongoDB
        cleaned_results = self._clean_for_mongodb(results)
        
        # Sauvegarde en base de donn√©es
        await self.db_manager.store_analysis_results(
            analysis_type="comprehensive_trend_analysis",
            results=cleaned_results
        )
        
        # Cr√©er les dossiers output s'ils n'existent pas
        from pathlib import Path
        analysis_dir = Path("output/analysis")
        analysis_dir.mkdir(parents=True, exist_ok=True)
        
        reports_dir = Path("output/reports")
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Timestamp pour les fichiers
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Sauvegarde en fichier JSON
        json_file = analysis_dir / f"analysis_results_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(cleaned_results, f, indent=2, ensure_ascii=False, default=str)
        
        self.logger.info(f"R√©sultats JSON sauvegard√©s dans {json_file}")
        
        # G√©n√©ration du rapport complet unique
        await self._generate_complete_report(results, reports_dir, timestamp)
    
    
    async def _generate_complete_report(self, results: Dict[str, Any], reports_dir, timestamp: str) -> None:
        """
        G√©n√®re un rapport complet incluant toutes les informations d'ex√©cution et d'analyse.
        
        Args:
            results: R√©sultats d'analyse complets
            reports_dir: R√©pertoire de sauvegarde des rapports
            timestamp: Timestamp pour nommer le fichier
        """
        try:
            # Rapport complet unique (format Markdown)
            complete_file = reports_dir / f"rapport_complet_{timestamp}.md"
            
            with open(complete_file, 'w', encoding='utf-8') as f:
                # En-t√™te du rapport
                f.write("# üìä RAPPORT COMPLET - STACK OVERFLOW SCRAPER & ANALYZER\n\n")
                f.write("*Rapport d'ex√©cution compl√®te du processus de scraping et d'analyse*\n\n")
                f.write("---\n\n")
                
                # 1. INFORMATIONS D'EX√âCUTION G√âN√âRALE
                self._write_execution_info(f, results)
                
                # 2. PHASE SCRAPING
                self._write_scraping_info(f, results)
                
                # 3. PHASE STOCKAGE
                self._write_storage_info(f, results)
                
                # 4. PHASE ANALYSE
                self._write_analysis_info(f, results)
                
                # 5. R√âSULTATS D'ANALYSE D√âTAILL√âS
                self._write_detailed_analysis_results(f, results)
                
                # 6. PERFORMANCE ET STATISTIQUES
                self._write_performance_stats(f, results)
                
                # 7. FOOTER
                f.write("\n---\n")
                f.write(f"**Rapport g√©n√©r√© le {datetime.now().strftime('%Y-%m-%d √† %H:%M:%S')}**\n")
                f.write("*Stack Overflow Scraper & Analyzer - Version compl√®te*\n")
            
            self.logger.info(f"üìÑ Rapport complet g√©n√©r√©: {complete_file}")
            
        except Exception as e:
            self.logger.error(f"Erreur lors de la g√©n√©ration du rapport complet: {e}")
    
    
    def _write_execution_info(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les informations d'ex√©cution g√©n√©rale."""
        f.write("## üöÄ INFORMATIONS D'EX√âCUTION G√âN√âRALE\n\n")
        
        # Informations de base
        if 'execution_info' in results:
            exec_info = results['execution_info']
            f.write("### Configuration d'ex√©cution\n\n")
            f.write(f"- **Date de d√©marrage**: {exec_info.get('start_time', 'N/A')}\n")
            f.write(f"- **Questions demand√©es**: {exec_info.get('max_questions', 'N/A')}\n")
            f.write(f"- **Tags cibl√©s**: {', '.join(exec_info.get('target_tags', [])) if exec_info.get('target_tags') else 'Tous les tags'}\n")
            f.write(f"- **Mode d'extraction**: {exec_info.get('extraction_mode', 'N/A')}\n")
            if 'total_duration' in exec_info:
                f.write(f"- **‚è±Ô∏è Temps total d'ex√©cution**: {exec_info['total_duration']:.2f} secondes\n")
            f.write("\n")
        
        # R√©sum√© des phases
        f.write("### R√©sum√© des phases d'ex√©cution\n\n")
        if 'execution_info' in results:
            exec_info = results['execution_info']
            f.write("| Phase | Dur√©e (s) | Status |\n")
            f.write("|-------|-----------|--------|\n")
            phases = [
                ('scraping', 'üîç Extraction'),
                ('storage', 'üíæ Stockage'),
                ('analysis', 'üìä Analyse')
            ]
            
            for phase_key, phase_name in phases:
                duration = exec_info.get(f'{phase_key}_duration', 'N/A')
                status = exec_info.get(f'{phase_key}_status', '‚úÖ Termin√©')
                if isinstance(duration, (int, float)):
                    f.write(f"| {phase_name} | {duration:.2f} | {status} |\n")
                else:
                    f.write(f"| {phase_name} | {duration} | {status} |\n")
            f.write("\n")
    
    def _write_scraping_info(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les informations de la phase de scraping."""
        f.write("## üîç PHASE 1: EXTRACTION DES DONN√âES\n\n")
        
        if 'execution_info' in results:
            exec_info = results['execution_info']
            f.write("### R√©sultats de l'extraction\n\n")
            f.write(f"- **Questions extraites**: {exec_info.get('questions_extracted', 'N/A')}\n")
            f.write(f"- **Auteurs uniques**: {exec_info.get('unique_authors', 'N/A')}\n")
            f.write(f"- **Tags uniques**: {exec_info.get('unique_tags', 'N/A')}\n")
            if 'extraction_rate' in exec_info:
                f.write(f"- **Taux d'extraction**: {exec_info['extraction_rate']:.1f} questions/sec\n")
            f.write("\n")
            
            # Erreurs et probl√®mes
            if exec_info.get('extraction_errors', 0) > 0:
                f.write("### ‚ö†Ô∏è Erreurs d'extraction\n\n")
                f.write(f"- **Erreurs rencontr√©es**: {exec_info['extraction_errors']}\n")
                f.write(f"- **Taux d'erreur**: {exec_info.get('error_rate', 0):.1f}%\n\n")
    
    def _write_storage_info(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les informations de la phase de stockage."""
        f.write("## üíæ PHASE 2: STOCKAGE EN BASE DE DONN√âES\n\n")
        
        if 'execution_info' in results:
            exec_info = results['execution_info']
            f.write("### Op√©rations de stockage\n\n")
            f.write(f"- **Questions stock√©es**: {exec_info.get('questions_stored', 'N/A')}\n")
            f.write(f"- **Auteurs stock√©s**: {exec_info.get('authors_stored', 'N/A')}\n")
            if 'storage_rate' in exec_info:
                f.write(f"- **Taux de stockage**: {exec_info['storage_rate']:.1f} questions/sec\n")
            f.write("\n")
    
    def _write_analysis_info(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les informations de la phase d'analyse."""
        f.write("## üìä PHASE 3: ANALYSE DES DONN√âES\n\n")
        
        if 'analysis_metadata' in results:
            meta = results['analysis_metadata']
            f.write("### Configuration de l'analyse\n\n")
            f.write(f"- **Date d'analyse**: {meta.get('analysis_date', 'N/A')}\n")
            f.write(f"- **Questions analys√©es**: {meta.get('total_questions', 0)}\n")
            f.write(f"- **Dur√©e de l'analyse**: {meta.get('duration', 0):.2f} secondes\n")
            if 'date_range' in meta and meta['date_range']['start']:
                f.write(f"- **P√©riode couverte**: {meta['date_range']['start']} √† {meta['date_range']['end']}\n")
            f.write("\n")
    
    def _write_detailed_analysis_results(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les r√©sultats d√©taill√©s de l'analyse."""
        f.write("## üìà R√âSULTATS D'ANALYSE D√âTAILL√âS\n\n")
        
        # Tags tendances
        if 'tag_trends' in results:
            f.write("### üè∑Ô∏è Analyse des Tags\n\n")
            if 'trending_tags' in results['tag_trends']:
                f.write("#### Tags en Tendance\n\n")
                f.write("| Rang | Tag | Croissance (%) | Questions Totales | Derni√®re Semaine | Dernier Mois |\n")
                f.write("|------|-----|----------------|-------------------|------------------|---------------|\n")
                for i, tag in enumerate(results['tag_trends']['trending_tags'][:15], 1):
                    f.write(f"| {i} | `{tag['tag']}` | {tag['growth_rate']:.1f}% | {tag['total_questions']} | {tag.get('last_week', 'N/A')} | {tag.get('last_month', 'N/A')} |\n")
                f.write("\n")
        
        # Patterns temporels
        if 'temporal_patterns' in results:
            f.write("### ‚è∞ Patterns Temporels\n\n")
            temporal = results['temporal_patterns']
            f.write("#### Activit√© par p√©riode\n\n")
            if 'peak_hour' in temporal:
                f.write(f"- **üïê Heure de pic d'activit√©**: {temporal['peak_hour']}h\n")
            if 'peak_day' in temporal:
                days = ['Lundi', 'Mardi', 'Mercredi', 'Jeudi', 'Vendredi', 'Samedi', 'Dimanche']
                day_name = days[temporal['peak_day']] if temporal['peak_day'] < 7 else f"Jour {temporal['peak_day']}"
                f.write(f"- **üìÖ Jour de pic d'activit√©**: {day_name}\n")
            f.write(f"- **üìä Questions analys√©es**: {temporal.get('total_questions_analyzed', 0)}\n")
            f.write("\n")
        
        # Analyse de contenu NLP
        if 'content_analysis' in results:
            f.write("### üìù Analyse de Contenu (NLP)\n\n")
            content = results['content_analysis']
            
            # Mots-cl√©s des titres
            if 'title_keywords' in content and content['title_keywords']:
                f.write("#### üî§ Mots-cl√©s des Titres (TF-IDF)\n\n")
                f.write("| Rang | Mot-cl√© | Score TF-IDF | Signification |\n")
                f.write("|------|---------|--------------|---------------|\n")
                for i, (keyword, score) in enumerate(content['title_keywords'][:15], 1):
                    significance = "Tr√®s important" if score > 0.5 else "Important" if score > 0.2 else "Mod√©r√©" if score > 0.1 else "Faible"
                    f.write(f"| {i} | `{keyword}` | {score:.3f} | {significance} |\n")
                f.write("\n")
                
                # Explication des scores TF-IDF
                f.write("üí° **Note**: Les scores TF-IDF mesurent l'importance statistique des mots dans le corpus. Un score plus √©lev√© indique un terme plus significatif et distinctif.\n\n")
            
            # Analyse de sentiment
            if 'title_sentiment' in content:
                f.write("#### üòä Analyse de Sentiment des Titres\n\n")
                sent = content['title_sentiment']
                total = sent.get('total', 1)
                if total > 0:
                    pos_pct = (sent.get('positive', 0) / total) * 100
                    neu_pct = (sent.get('neutral', 0) / total) * 100
                    neg_pct = (sent.get('negative', 0) / total) * 100
                    
                    f.write("| Sentiment | Nombre | Pourcentage |\n")
                    f.write("|-----------|--------|--------------|\n")
                    f.write(f"| üòä **Positif** | {sent.get('positive', 0)} | {pos_pct:.1f}% |\n")
                    f.write(f"| üòê **Neutre** | {sent.get('neutral', 0)} | {neu_pct:.1f}% |\n")
                    f.write(f"| üòû **N√©gatif** | {sent.get('negative', 0)} | {neg_pct:.1f}% |\n")
                    f.write(f"| üìä **Score moyen** | - | {sent.get('average', 0):.3f} |\n")
                    f.write("\n")
                    
                    # Explication du score de sentiment
                    f.write("üí° **Note**: Le score de sentiment varie de -1 (tr√®s n√©gatif) √† +1 (tr√®s positif). Un score proche de 0 indique un contenu neutre/technique.\n\n")
        
        # Analyse des auteurs
        if 'author_analysis' in results and 'top_authors' in results['author_analysis']:
            f.write("### üë• Analyse des Auteurs\n\n")
            f.write("#### Top Contributeurs\n\n")
            f.write("| Rang | Auteur | R√©putation | Questions | Activit√© |\n")
            f.write("|------|--------|------------|-----------|----------|\n")
            for i, author in enumerate(results['author_analysis']['top_authors'][:10], 1):
                author_name = author.get('author_name', author.get('name', 'N/A'))
                reputation = author.get('reputation', 0)
                questions = author.get('question_count', 0)
                activity = "üî• Tr√®s actif" if questions >= 10 else "‚ö° Actif" if questions >= 5 else "üìù Contributeur"
                f.write(f"| {i} | **{author_name}** | {reputation:,} | {questions} | {activity} |\n")
            f.write("\n")
            
            # Statistiques des auteurs
            if 'reputation_stats' in results['author_analysis']:
                rep_stats = results['author_analysis']['reputation_stats']
                f.write("#### üìä Statistiques de R√©putation\n\n")
                f.write(f"- **R√©putation moyenne**: {rep_stats.get('mean', 0):.0f} points\n")
                f.write(f"- **R√©putation m√©diane**: {rep_stats.get('median', 0):.0f} points\n")
                f.write(f"- **R√©putation maximale**: {rep_stats.get('max', 0):,} points\n")
                f.write(f"- **R√©putation minimale**: {rep_stats.get('min', 0):,} points\n")
                f.write("\n")
    
    def _write_performance_stats(self, f, results: Dict[str, Any]) -> None:
        """√âcrit les statistiques de performance et g√©n√©rales."""
        f.write("## üìä STATISTIQUES G√âN√âRALES ET PERFORMANCE\n\n")
        
        if 'general_stats' in results:
            stats = results['general_stats']
            
            # Statistiques des questions
            f.write("### üìã Statistiques des Questions\n\n")
            if 'vote_stats' in stats:
                vote_stats = stats['vote_stats']
                f.write("#### üëç Votes et Scores\n")
                f.write(f"- **Score moyen**: {vote_stats.get('mean', 0):.2f}\n")
                f.write(f"- **Score m√©dian**: {vote_stats.get('median', 0):.2f}\n")
                f.write(f"- **Score maximum**: {vote_stats.get('max', 0)}\n")
                f.write(f"- **√âcart-type**: {vote_stats.get('std', 0):.2f}\n\n")
            
            if 'view_stats' in stats:
                view_stats = stats['view_stats']
                f.write("#### üëÄ Vues et Visibilit√©\n")
                f.write(f"- **Vues moyennes**: {view_stats.get('mean', 0):.0f}\n")
                f.write(f"- **Vues m√©dianes**: {view_stats.get('median', 0):.0f}\n")
                f.write(f"- **Vues maximum**: {view_stats.get('max', 0):,}\n")
                f.write(f"- **√âcart-type**: {view_stats.get('std', 0):.0f}\n\n")
            
            if 'answer_stats' in stats:
                answer_stats = stats['answer_stats']
                f.write("#### üí¨ R√©ponses et Engagement\n")
                f.write(f"- **R√©ponses moyennes**: {answer_stats.get('mean', 0):.2f}\n")
                f.write(f"- **R√©ponses m√©dianes**: {answer_stats.get('median', 0):.2f}\n")
                f.write(f"- **R√©ponses maximum**: {answer_stats.get('max', 0)}\n")
                f.write(f"- **Taux sans r√©ponse**: {answer_stats.get('unanswered_rate', 0):.1f}%\n\n")
            
            # Statistiques des tags
            if 'tag_stats' in stats:
                tag_stats = stats['tag_stats']
                f.write("### üè∑Ô∏è Statistiques des Tags\n\n")
                f.write(f"- **Tags uniques**: {tag_stats.get('total_unique_tags', 0)}\n")
                f.write(f"- **Tags par question (moyenne)**: {tag_stats.get('average_tags_per_question', 0):.2f}\n\n")
                
                if 'most_common_tags' in tag_stats:
                    f.write("#### Top Tags par Popularit√©\n\n")
                    f.write("| Rang | Tag | Questions |\n")
                    f.write("|------|-----|----------|\n")
                    for i, (tag, count) in enumerate(tag_stats['most_common_tags'][:15], 1):
                        f.write(f"| {i} | `{tag}` | {count} |\n")
                    f.write("\n")
        
        # Performance syst√®me
        if 'execution_info' in results:
            exec_info = results['execution_info']
            f.write("### ‚ö° Performance du Syst√®me\n\n")
            if 'total_duration' in exec_info:
                total_time = exec_info['total_duration']
                questions = exec_info.get('questions_extracted', 0)
                if questions > 0:
                    f.write(f"- **Efficacit√© globale**: {questions/total_time:.1f} questions/seconde\n")
                f.write(f"- **Temps total d'ex√©cution**: {total_time:.2f} secondes\n")
                
                # R√©partition du temps par phase
                phases = ['scraping', 'storage', 'analysis']
                f.write("\n#### R√©partition du temps par phase\n\n")
                f.write("| Phase | Temps (s) | Pourcentage |\n")
                f.write("|-------|-----------|-------------|\n")
                for phase in phases:
                    duration = exec_info.get(f'{phase}_duration', 0)
                    if isinstance(duration, (int, float)) and total_time > 0:
                        percentage = (duration / total_time) * 100
                        phase_name = {'scraping': 'üîç Extraction', 'storage': 'üíæ Stockage', 'analysis': 'üìä Analyse'}.get(phase, phase)
                        f.write(f"| {phase_name} | {duration:.2f} | {percentage:.1f}% |\n")
                f.write("\n")
                f.write("| Rang | Auteur | R√©putation | Questions |\n")
                f.write("|------|--------|------------|----------|\n")
