# ğŸ” Stack Overflow Scraper

Un outil complet d'extraction et d'analyse de donnÃ©es Stack Overflow avec support dual (Web Scraping + API).

## ğŸ“‹ Table des matiÃ¨res

- [FonctionnalitÃ©s](#-fonctionnalitÃ©s)
- [Installation](#-installation)
- [Configuration](#-configuration)
- [Utilisation](#-utilisation)
- [ParamÃ¨tres et options](#-paramÃ¨tres-et-options)
- [Exemples d'utilisation](#-exemples-dutilisation)
- [Structure des donnÃ©es](#-structure-des-donnÃ©es)
- [Tests](#-tests)
- [DÃ©pannage](#-dÃ©pannage)

## âœ¨ FonctionnalitÃ©s

- **Extraction de donnÃ©es** : Scraping web (Selenium) ou API officielle Stack Overflow
- **Stockage intelligent** : Base de donnÃ©es MongoDB avec gestion des doublons
- **Analyse complÃ¨te** : NLP, analyse de sentiment, dÃ©tection de tendances
- **Rapports dÃ©taillÃ©s** : GÃ©nÃ©ration automatique de rapports complets
- **Logging avancÃ©** : Suivi complet de l'exÃ©cution avec mÃ©triques

## ğŸš€ Installation

### PrÃ©requis

- Python 3.8+
- MongoDB (local ou distant)
- Google Chrome (pour le scraping web)

### Installation des dÃ©pendances

```bash
pip install -r requirements.txt
```

### Configuration initiale

1. **DÃ©marrer MongoDB** (si local) :
   ```bash
   mongod
   ```

2. **VÃ©rifier la connexion MongoDB** :
   ```bash
   python utils/check_mongodb.py
   ```

3. **Configurer l'environnement** (optionnel) :
   ```bash
   cp .env.example .env
   # Ã‰diter .env selon vos besoins
   ```

## âš™ï¸ Configuration

Le scraper utilise plusieurs sources de configuration par ordre de prioritÃ© :

1. **Variables d'environnement** (prioritÃ© maximale)
2. **Fichier config.json** 
3. **Valeurs par dÃ©faut**

### Configuration MongoDB

```json
{
  "database": {
    "host": "localhost",
    "port": 27017,
    "name": "stackoverflow_data",
    "collection": "questions"
  }
}
```

### Configuration API Stack Overflow

```json
{
  "api": {
    "key": "",  // Optionnel - 10k requÃªtes/jour sans clÃ©
    "rate_limit": 300,
    "site": "stackoverflow"
  }
}
```

## ğŸ¯ Utilisation

### Commande de base

```bash
python main.py
```

Cette commande :
- Extrait 300 questions (par dÃ©faut)
- Utilise le scraping web
- Stocke les donnÃ©es en MongoDB
- GÃ©nÃ¨re une analyse complÃ¨te

### Syntaxe complÃ¨te

```bash
python main.py [OPTIONS]
```

## ğŸ“ ParamÃ¨tres et options

| Option | Raccourci | Type | DÃ©faut | Description |
|--------|-----------|------|--------|-------------|
| `--max-questions` | `-n` | int | 300 | Nombre max de questions Ã  extraire |
| `--tags` | `-t` | list | None | Tags Ã  filtrer (ex: python javascript) |
| `--use-api` | | flag | False | Utiliser l'API au lieu du scraping |
| `--no-analysis` | | flag | False | DÃ©sactiver l'analyse des donnÃ©es |
| `--log-level` | | choice | INFO | Niveau de logging (DEBUG/INFO/WARNING/ERROR) |

## ğŸ’¡ Exemples d'utilisation

### 1. Extraction basique (300 questions)

```bash
python main.py
```

### 2. Extraction ciblÃ©e par tags

```bash
# Questions Python et JavaScript
python main.py --tags python javascript

# Questions React seulement
python main.py -t react
```

### 3. Extraction en volume

```bash
# 1000 questions via scraping web
python main.py --max-questions 1000

# 500 questions via API (plus rapide)
python main.py -n 500 --use-api
```

### 4. Extraction sans analyse

```bash
# Extraction seule (sans analyse)
python main.py --no-analysis
```

### 5. Mode dÃ©bogage

```bash
# Logs dÃ©taillÃ©s pour dÃ©pannage
python main.py --log-level DEBUG
```

### 6. Combinaisons avancÃ©es

```bash
# 2500 questions Python via API avec logs dÃ©taillÃ©s
python main.py -n 2500 -t python --use-api --log-level DEBUG

# Extraction rapide React + Vue sans analyse
python main.py -t react vue.js --use-api --no-analysis
```

## ğŸ“Š Structure des donnÃ©es

### Question extraite

```python
{
    "title": "How to use async/await in Python?",
    "url": "https://stackoverflow.com/questions/...",
    "summary": "I'm trying to understand async programming...",
    "tags": ["python", "async-await", "asyncio"],
    "author_name": "PythonDev",
    "author_reputation": 5420,
    "publication_date": "2025-08-06T10:30:00",
    "view_count": 1250,
    "vote_count": 15,
    "answer_count": 3,
    "question_id": 78901234
}
```

### Rapport d'analyse gÃ©nÃ©rÃ©

- **Statistiques gÃ©nÃ©rales** : Nombre de questions, auteurs, tags
- **Analyse temporelle** : Tendances par pÃ©riode
- **Analyse des tags** : Tags les plus populaires
- **Analyse de sentiment** : Sentiment moyen des questions
- **Mots-clÃ©s** : Extraction TF-IDF des termes importants
- **MÃ©triques d'exÃ©cution** : Temps, taux d'extraction, performance

## ğŸ§ª Tests

### Lancer tous les tests

```bash
python run_tests.py
```

### Tests spÃ©cifiques

```bash
# Tests du scraper seulement
pytest tests/test_scraper.py -v

# Tests avec couverture
pytest tests/ --cov=src
```

### Rapport de tests

Les rapports de tests sont automatiquement gÃ©nÃ©rÃ©s dans `output/reports/` avec :
- Statistiques dÃ©taillÃ©es
- Tests Ã©chouÃ©s/rÃ©ussis
- Temps d'exÃ©cution
- Recommandations

## ğŸ”§ DÃ©pannage

### ProblÃ¨mes courants

#### 1. Erreur de connexion MongoDB

```bash
# VÃ©rifier que MongoDB est dÃ©marrÃ©
python utils/check_mongodb.py

# Nettoyer la base si nÃ©cessaire
python utils/clear_database.py
```

#### 2. Erreur Chrome/Selenium

```bash
# Installer/mettre Ã  jour ChromeDriver
pip install --upgrade webdriver-manager
```

#### 3. Erreur de dÃ©pendances NLP

```bash
# Installer les ressources NLTK
python -c "import nltk; nltk.download('punkt'); nltk.download('stopwords'); nltk.download('wordnet')"
```

#### 4. Erreur de taux limite API

```bash
# Utiliser le mode scraping web
python main.py --max-questions 100  # sans --use-api

# Ou rÃ©duire le nombre de questions
python main.py --use-api -n 50
```

### Logs de dÃ©bogage

```bash
# Logs dÃ©taillÃ©s
python main.py --log-level DEBUG

# Logs dans le fichier
tail -f logs/scraper.log
```

### Nettoyage de la base de donnÃ©es

```bash
# âš ï¸  ATTENTION: Supprime toutes les donnÃ©es
python utils/clear_database.py
```

## ğŸ“ Structure du projet

```
so-scrapper/
â”œâ”€â”€ src/                    # Code source principal
â”‚   â”œâ”€â”€ scraper.py         # Module de scraping
â”‚   â”œâ”€â”€ database.py        # Gestion MongoDB
â”‚   â”œâ”€â”€ analyzer.py        # Analyse des donnÃ©es
â”‚   â””â”€â”€ config.py          # Configuration
â”œâ”€â”€ tests/                 # Tests unitaires
â”œâ”€â”€ utils/                 # Utilitaires
â”œâ”€â”€ output/               # Rapports gÃ©nÃ©rÃ©s
â”‚   â”œâ”€â”€ analysis/         # Analyses de donnÃ©es
â”‚   â””â”€â”€ reports/          # Rapports de tests
â”œâ”€â”€ logs/                 # Fichiers de log
â”œâ”€â”€ main.py              # Point d'entrÃ©e principal
â”œâ”€â”€ run_tests.py         # Script de tests
â”œâ”€â”€ config.json          # Configuration
â””â”€â”€ requirements.txt     # DÃ©pendances
```

## ğŸš€ Workflow typique

1. **Extraction** â†’ RÃ©cupÃ©ration des questions Stack Overflow
2. **Stockage** â†’ Sauvegarde en MongoDB avec gestion des doublons
3. **Analyse** â†’ Traitement NLP et identification des tendances
4. **Rapport** â†’ GÃ©nÃ©ration automatique du rapport complet

## ğŸ“„ Licence

Ce projet est sous licence MIT. Voir le fichier `LICENSE` pour plus de dÃ©tails.

---

**Auteur** : Pierre Mazard  
**Date** : AoÃ»t 2025  
**Version** : 1.0.0