#!/usr/bin/env python3
"""
Stack Overflow Scraper - Main Script
====================================

Point d'entr√©e principal pour l'outil d'extraction de donn√©es Stack Overflow.
Ce script orchestre le processus de scraping, stockage et analyse des donn√©es.

Usage:
    python main.py [options]

Author: Pierre Mazard
Date: August 2025
"""

import argparse
import asyncio
import logging
from datetime import datetime
from typing import Optional, List, Dict

from src.scraper import StackOverflowScraper, QuestionData
from src.database import DatabaseManager
from src.analyzer import DataAnalyzer
from src.config import Config


async def store_questions_update_only(db_manager: DatabaseManager, questions_data: List[QuestionData], logger) -> Dict[str, int]:
    """
    Stocke les questions en mode 'update-only' : met √† jour seulement les questions/auteurs existants.
    
    Args:
        db_manager: Gestionnaire de base de donn√©es
        questions_data: Liste des questions √† traiter
        logger: Logger pour les messages
        
    Returns:
        Dict avec les statistiques de stockage
    """
    logger.info("üîç Mode UPDATE-ONLY : Mise √† jour des questions existantes uniquement...")
    
    # R√©cup√©rer les IDs existants
    questions_coll = db_manager.motor_database[db_manager.questions_collection]
    existing_ids = set()
    
    async for doc in questions_coll.find({}, {"question_id": 1}):
        existing_ids.add(doc["question_id"])
    
    logger.info(f"üìä Questions existantes en base: {len(existing_ids)}")
    
    # Filtrer les questions existantes √† mettre √† jour
    update_questions = []
    for question in questions_data:
        if question.question_id in existing_ids:
            update_questions.append(question)
    
    logger.info(f"üîÑ Questions √† mettre √† jour: {len(update_questions)}")
    logger.info(f"üö´ Questions nouvelles ignor√©es: {len(questions_data) - len(update_questions)}")
    
    if update_questions:
        # Mettre √† jour uniquement les existantes
        return await db_manager.store_questions(update_questions, update_only=True)
    else:
        logger.info("‚ÑπÔ∏è  Aucune question existante √† mettre √† jour")
        return {'questions_stored': 0, 'authors_new': 0, 'authors_updated': 0}


async def store_questions_append_only(db_manager: DatabaseManager, questions_data: List[QuestionData], logger) -> Dict[str, int]:
    """
    Stocke les questions en mode 'append-only' : ignore compl√®tement les doublons.
    
    Args:
        db_manager: Gestionnaire de base de donn√©es
        questions_data: Liste des questions √† stocker
        logger: Logger pour les messages
        
    Returns:
        Dict avec les statistiques de stockage
    """
    logger.info("üîç Mode APPEND-ONLY : Filtrage des questions existantes...")
    
    # R√©cup√©rer les IDs existants
    questions_coll = db_manager.motor_database[db_manager.questions_collection]
    existing_ids = set()
    
    async for doc in questions_coll.find({}, {"question_id": 1}):
        existing_ids.add(doc["question_id"])
    
    logger.info(f"üìä Questions existantes en base: {len(existing_ids)}")
    
    # Filtrer les nouvelles questions
    new_questions = []
    for question in questions_data:
        if question.question_id not in existing_ids:
            new_questions.append(question)
    
    logger.info(f"‚ú® Nouvelles questions √† ajouter: {len(new_questions)}")
    logger.info(f"üö´ Questions doublons ignor√©es: {len(questions_data) - len(new_questions)}")
    
    if new_questions:
        # Stocker uniquement les nouvelles
        return await db_manager.store_questions(new_questions)
    else:
        logger.info("‚ÑπÔ∏è  Aucune nouvelle question √† stocker")
        return {'questions_stored': 0, 'authors_new': 0, 'authors_updated': 0}


def setup_logging(log_level: str = "INFO") -> None:
    """Configure le syst√®me de logging."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/scraper.log'),
            logging.StreamHandler()
        ]
    )


async def main(
    max_questions: int = 100,
    tags: Optional[list] = None,
    use_api: bool = False,
    analyze_data: bool = True,
    storage_mode: str = "upsert",
    analysis_scope: str = "all"
) -> None:
    """
    Fonction principale pour ex√©cuter le scraping et l'analyse.
    
    Args:
        max_questions: Nombre maximum de questions √† scraper
        tags: Liste des tags √† filtrer
        use_api: Utiliser l'API Stack Overflow au lieu du scraping
        analyze_data: Effectuer l'analyse des donn√©es apr√®s extraction
        storage_mode: Mode de stockage ("upsert", "update", "append-only")
        analysis_scope: Port√©e de l'analyse ("all", "new-only")
    """
    logger = logging.getLogger(__name__)
    logger.info("[START] D√âMARRAGE DU STACK OVERFLOW SCRAPER")
    logger.info("=" * 60)
    
    # Dictionnaire pour collecter les informations d'ex√©cution
    execution_info = {
        'start_time': datetime.now().isoformat(),
        'max_questions': max_questions,
        'target_tags': tags or [],
        'extraction_mode': 'API Stack Overflow' if use_api else 'Scraping web',
        'storage_mode': storage_mode,
        'analysis_scope': analysis_scope
    }
    
    try:
        # Initialisation des composants
        logger.info("[INIT]  PHASE 0: Initialisation des composants...")
        config = Config()
        logger.info("[OK] Configuration charg√©e")
        
        db_manager = DatabaseManager(config.database_config)
        await db_manager.connect()  # Connexion √† la base de donn√©es
        logger.info("[OK] Connexion √† la base de donn√©es √©tablie")
        
        # Afficher le mode de stockage choisi
        storage_modes = {
            "upsert": "üîÑ Insert + Mise √† jour (upsert - ajoute les nouvelles, met √† jour les existantes)",
            "update": "üîÑ Mise √† jour uniquement (met √† jour seulement les questions/auteurs existants)",
            "append-only": "‚ûï Ajout uniquement (filtre les questions existantes, ajoute seulement les nouvelles)"
        }
        logger.info(f"[MODE] {storage_modes.get(storage_mode, storage_mode)}")
        
        scraper = StackOverflowScraper({
            **config.scraper_config.__dict__,
            'api': config.api_config.__dict__
        })
        await scraper.setup_session()  # Initialisation de la session
        logger.info("[OK] Session de scraping initialis√©e")
        logger.info("[READY] Initialisation termin√©e - D√©but du processus principal...")
        logger.info("-" * 60)
        
        # Extraction des donn√©es
        logger.info(f"[EXTRACT] PHASE 1: Extraction de {max_questions} questions...")
        logger.info(f"Tags cibl√©s: {tags if tags else 'Tous les tags'}")
        logger.info(f"Mode: {execution_info['extraction_mode']}")
        
        start_time = datetime.now()
        scraping_start = start_time
        if use_api:
            questions_data = await scraper.fetch_via_api(
                max_questions=max_questions,
                tags=tags
            )
        else:
            questions_data = await scraper.scrape_questions(
                max_questions=max_questions,
                tags=tags
            )
        
        extraction_time = datetime.now() - scraping_start
        execution_info.update({
            'scraping_duration': extraction_time.total_seconds(),
            'questions_extracted': len(questions_data),
            'extraction_rate': len(questions_data) / extraction_time.total_seconds() if extraction_time.total_seconds() > 0 else 0,
            'scraping_status': '‚úÖ Termin√©'
        })
        
        # Calcul des statistiques d'extraction
        unique_authors = set()
        unique_tags = set()
        for question in questions_data:
            if question.author_name:
                unique_authors.add(question.author_name)
            if question.tags:
                unique_tags.update(question.tags)
        
        execution_info.update({
            'unique_authors': len(unique_authors),
            'unique_tags': len(unique_tags)
        })
        
        logger.info(f"[OK] Extraction termin√©e: {len(questions_data)} questions r√©cup√©r√©es en {extraction_time.total_seconds():.1f}s")
        
        # Stockage en base de donn√©es avec mode intelligent
        logger.info("[STORE] PHASE 2: Stockage des donn√©es en base...")
        logger.info(f"Donn√©es √† stocker: {len(questions_data)} questions")
        logger.info(f"Mode de stockage: {storage_mode}")
        
        storage_start = datetime.now()
        new_questions_ids = []  # Pour traquer les nouvelles questions
        
        if storage_mode == "upsert":
            # Mode par d√©faut : insert + mise √† jour (upsert)
            storage_result = await db_manager.store_questions(questions_data)
            stored_count = storage_result['questions_stored']
            authors_new = storage_result['authors_new']
            authors_updated = storage_result['authors_updated']
            # En mode upsert, on consid√®re toutes les questions comme "nouvelles" pour l'analyse
            new_questions_ids = [q.question_id for q in questions_data]
            
        elif storage_mode == "update":
            # Mode mise √† jour uniquement : met √† jour seulement les existantes
            storage_result = await store_questions_update_only(db_manager, questions_data, logger)
            stored_count = storage_result['questions_stored']
            authors_new = storage_result['authors_new']
            authors_updated = storage_result['authors_updated']
            # En mode update, seules les questions mises √† jour sont consid√©r√©es comme "nouvelles" pour l'analyse
            existing_ids = await db_manager.get_question_ids()
            new_questions_ids = [q.question_id for q in questions_data if q.question_id in existing_ids]
            
        elif storage_mode == "append-only":
            # Mode ajout uniquement : ignore les doublons
            # R√©cup√©rer les IDs existants pour identifier les nouvelles
            existing_ids = await db_manager.get_question_ids()
            before_count = len(existing_ids)
            
            storage_result = await store_questions_append_only(db_manager, questions_data, logger)
            stored_count = storage_result['questions_stored']
            authors_new = storage_result['authors_new']
            authors_updated = storage_result['authors_updated']
            
            # Identifier les nouvelles questions (celles qui ont √©t√© r√©ellement ajout√©es)
            new_questions_ids = [q.question_id for q in questions_data if q.question_id not in existing_ids]
            
        execution_info.update({
            'new_questions_count': len(new_questions_ids),
            'new_questions_ids': new_questions_ids
        })
        
        storage_time = datetime.now() - storage_start
        
        # Log des d√©tails d'auteurs 
        if authors_new > 0:
            logger.info(f"üë• Nouveaux auteurs ajout√©s: {authors_new}")
        if authors_updated > 0:
            logger.info(f"üîÑ Auteurs mis √† jour: {authors_updated}")
        if authors_new == 0 and authors_updated == 0 and stored_count == 0:
            logger.info("‚ÑπÔ∏è  Aucun auteur ajout√© ou mis √† jour")
        
        execution_info.update({
            'storage_duration': storage_time.total_seconds(),
            'questions_stored': stored_count,
            'questions_attempted': len(questions_data),
            'authors_new': authors_new,
            'authors_updated': authors_updated,
            'authors_total_affected': authors_new + authors_updated,
            'storage_rate': stored_count / storage_time.total_seconds() if storage_time.total_seconds() > 0 else 0,
            'storage_status': '‚úÖ Termin√©',
            'storage_mode': storage_mode
        })
        
        logger.info(f"[OK] Stockage termin√© en {storage_time.total_seconds():.1f}s")
        logger.info(f"üìä Bilan: {stored_count}/{len(questions_data)} questions stock√©es")
        
        # Analyse des donn√©es
        if analyze_data:
            logger.info("[ANALYZE] PHASE 3: Analyse des donn√©es...")
            logger.info("Initialisation de l'analyseur...")
            
            analyzer = DataAnalyzer(db_manager)
            
            # Passer les informations d'ex√©cution √† l'analyseur
            total_time_so_far = (datetime.now() - start_time).total_seconds()
            execution_info['total_duration_so_far'] = total_time_so_far
            analyzer.set_execution_metadata(execution_info)
            
            # D√©terminer les questions √† analyser selon l'analysis_scope
            questions_to_analyze = None
            if analysis_scope == "new-only":
                if new_questions_ids:
                    questions_to_analyze = new_questions_ids
                    logger.info(f"üéØ Analyse limit√©e aux {len(new_questions_ids)} nouvelles questions")
                else:
                    logger.warning("‚ö†Ô∏è Aucune nouvelle question trouv√©e, analyse annul√©e")
                    execution_info.update({
                        'analysis_status': '‚ö†Ô∏è Annul√©e - Aucune nouvelle question',
                        'analysis_scope': analysis_scope
                    })
                    logger.info("‚è≠Ô∏è  Analyse annul√©e")
                    questions_to_analyze = "skip"
            else:
                logger.info("üéØ Analyse de toutes les questions disponibles")
            
            if questions_to_analyze != "skip":
                analysis_start = datetime.now()
                logger.info("D√©marrage de l'analyse des tendances...")
                
                # Passer les IDs des questions √† analyser (None = toutes les questions)
                analysis_results = await analyzer.analyze_trends(question_ids=questions_to_analyze)
                analysis_time = datetime.now() - analysis_start
                
                analyzed_count = len(questions_to_analyze) if questions_to_analyze else "toutes"
                execution_info.update({
                    'analysis_duration': analysis_time.total_seconds(),
                    'analysis_status': '‚úÖ Termin√©',
                    'analysis_scope': analysis_scope,
                    'analyzed_questions_count': analyzed_count
                })
                
                logger.info(f"[OK] Analyse termin√©e en {analysis_time.total_seconds():.1f}s")
                
                # Sauvegarde des r√©sultats d'analyse (sans visualisations)
                logger.info("[SAVE] Sauvegarde des r√©sultats...")
                save_start = datetime.now()
                await analyzer.save_results(analysis_results)
                save_time = datetime.now() - save_start
                
                execution_info.update({
                    'save_duration': save_time.total_seconds()
                })
                
                logger.info(f"[OK] Sauvegarde termin√©e en {save_time.total_seconds():.1f}s")
            else:
                # G√©n√©rer un rapport m√™me si l'analyse est annul√©e
                logger.info("[REPORT] G√©n√©ration d'un rapport d'ex√©cution...")
                save_start = datetime.now()
                
                # Cr√©er un r√©sultat vide avec les informations d'ex√©cution
                empty_results = {
                    'execution_info': execution_info,
                    'analysis_skipped': True,
                    'skip_reason': execution_info.get('analysis_skipped_reason', 'Aucune nouvelle question √† analyser')
                }
                
                await analyzer.save_results(empty_results)
                save_time = datetime.now() - save_start
                
                execution_info.update({
                    'save_duration': save_time.total_seconds()
                })
                
                logger.info(f"[OK] Rapport d'ex√©cution g√©n√©r√© en {save_time.total_seconds():.1f}s")
        else:
            logger.info("‚è≠Ô∏è  Analyse des donn√©es d√©sactiv√©e")
            
            # Modifier les informations d'ex√©cution pour refl√©ter que l'analyse est d√©sactiv√©e
            execution_info['analysis_scope'] = 'disabled'
            
            # G√©n√©rer un rapport m√™me si l'analyse est d√©sactiv√©e
            logger.info("[REPORT] G√©n√©ration d'un rapport d'ex√©cution...")
            save_start = datetime.now()
            
            # Cr√©er un DataAnalyzer pour g√©n√©rer le rapport
            analyzer = DataAnalyzer(db_manager)
            analyzer.set_execution_metadata(execution_info)
            
            # Cr√©er un r√©sultat vide avec les informations d'ex√©cution
            disabled_results = {
                'execution_info': execution_info,
                'analysis_disabled': True,
                'skip_reason': 'Analyse d√©sactiv√©e par l\'utilisateur (--no-analysis)'
            }
            
            await analyzer.save_results(disabled_results)
            save_time = datetime.now() - save_start
            
            execution_info.update({
                'save_duration': save_time.total_seconds(),
                'analysis_status': '‚è≠Ô∏è D√©sactiv√©e'
            })
            
            logger.info(f"[OK] Rapport d'ex√©cution g√©n√©r√© en {save_time.total_seconds():.1f}s")
            execution_info.update({
                'analysis_status': '‚è≠Ô∏è D√©sactiv√©e'
            })
        
        total_time = datetime.now() - start_time
        execution_info['total_duration'] = total_time.total_seconds()
        
        logger.info("üéâ PROCESSUS TERMIN√â AVEC SUCC√àS!")
        logger.info(f"Temps total:  Temps total d'ex√©cution: {total_time.total_seconds():.1f}s")
        logger.info(f"Questions extraites: R√©sum√©: {len(questions_data)} questions trait√©es")
        
        # Fermeture des connexions
        logger.info("üîå Fermeture des connexions...")
        await db_manager.disconnect()
        await scraper.close()
        logger.info("[OK] Toutes les connexions ferm√©es")
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ex√©cution : {e}")
        # Nettoyage en cas d'erreur
        try:
            if 'db_manager' in locals():
                await db_manager.disconnect()
            if 'scraper' in locals():
                await scraper.close()
        except:
            pass
        raise


def parse_arguments():
    """Parse les arguments de ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Stack Overflow Data Scraper and Analyzer"
    )
    
    parser.add_argument(
        "--max-questions", "-n",
        type=int,
        default=300,
        help="Nombre maximum de questions √† extraire (d√©faut: 300)"
    )
    
    parser.add_argument(
        "--tags", "-t",
        nargs="+",
        help="Tags √† filtrer (ex: python javascript react)"
    )
    
    parser.add_argument(
        "--use-api",
        action="store_true",
        help="Utiliser l'API Stack Overflow au lieu du scraping"
    )
    
    parser.add_argument(
        "--no-analysis",
        action="store_true",
        help="D√©sactiver l'analyse des donn√©es"
    )
    
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Niveau de logging (d√©faut: INFO)"
    )
    
    parser.add_argument(
        "--mode",
        choices=["upsert", "update", "append-only"],
        default="upsert",
        help="Mode de stockage des questions (d√©faut: upsert)\n"
             "upsert: Insert nouvelles + met √† jour existantes (comportement par d√©faut)\n"
             "update: Met √† jour seulement les questions/auteurs existants (pas d'ajout)\n"
             "append-only: Ajoute seulement les nouvelles questions (filtre les existantes)"
    )
    
    parser.add_argument(
        "--analysis-scope",
        choices=["all", "new-only"],
        default="all",
        help="Port√©e de l'analyse (d√©faut: all)\n"
             "all: Analyse toutes les questions dans la base de donn√©es\n"
             "new-only: Analyse seulement les questions nouvellement ajout√©es/mises √† jour"
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()
    setup_logging(args.log_level)
    
    # Ex√©cution asynchrone du scraper
    asyncio.run(main(
        max_questions=args.max_questions,
        tags=args.tags,
        use_api=args.use_api,
        analyze_data=not args.no_analysis,
        storage_mode=args.mode,
        analysis_scope=args.analysis_scope
    ))
