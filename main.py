#!/usr/bin/env python3
"""
Stack Overflow Scraper - Main Script
====================================

Point d'entr√©e principal pour l'outil d'extraction de donn√©es Stack Overflow.
Ce script orchestre le processus de scraping, stockage et analyse des donn√©es.

Usage:
    python main.py [options]

Author: Pierre Mazard
Date: August 2025
"""

import argparse
import asyncio
import logging
from datetime import datetime
from typing import Optional

from src.scraper import StackOverflowScraper
from src.database import DatabaseManager
from src.analyzer import DataAnalyzer
from src.config import Config


def setup_logging(log_level: str = "INFO") -> None:
    """Configure le syst√®me de logging."""
    logging.basicConfig(
        level=getattr(logging, log_level.upper()),
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        handlers=[
            logging.FileHandler('logs/scraper.log'),
            logging.StreamHandler()
        ]
    )


async def main(
    max_questions: int = 100,
    tags: Optional[list] = None,
    use_api: bool = False,
    analyze_data: bool = True
) -> None:
    """
    Fonction principale pour ex√©cuter le scraping et l'analyse.
    
    Args:
        max_questions: Nombre maximum de questions √† scraper
        tags: Liste des tags √† filtrer
        use_api: Utiliser l'API Stack Overflow au lieu du scraping
        analyze_data: Effectuer l'analyse des donn√©es apr√®s extraction
    """
    logger = logging.getLogger(__name__)
    logger.info("[START] D√âMARRAGE DU STACK OVERFLOW SCRAPER")
    logger.info("=" * 60)
    
    try:
        # Initialisation des composants
        logger.info("[INIT]  PHASE 0: Initialisation des composants...")
        config = Config()
        logger.info("[OK] Configuration charg√©e")
        
        db_manager = DatabaseManager(config.database_config)
        await db_manager.connect()  # Connexion √† la base de donn√©es
        logger.info("[OK] Connexion √† la base de donn√©es √©tablie")
        
        scraper = StackOverflowScraper(config.scraper_config)
        await scraper.setup_session()  # Initialisation de la session
        logger.info("[OK] Session de scraping initialis√©e")
        logger.info("[READY] Initialisation termin√©e - D√©but du processus principal...")
        logger.info("-" * 60)
        
        # Extraction des donn√©es
        logger.info(f"[EXTRACT] PHASE 1: Extraction de {max_questions} questions...")
        logger.info(f"Tags cibl√©s: {tags if tags else 'Tous les tags'}")
        logger.info(f"Mode: {'API Stack Overflow' if use_api else 'Scraping web'}")
        
        start_time = datetime.now()
        if use_api:
            questions_data = await scraper.fetch_via_api(
                max_questions=max_questions,
                tags=tags
            )
        else:
            questions_data = await scraper.scrape_questions(
                max_questions=max_questions,
                tags=tags
            )
        
        extraction_time = datetime.now() - start_time
        logger.info(f"[OK] Extraction termin√©e: {len(questions_data)} questions r√©cup√©r√©es en {extraction_time.total_seconds():.1f}s")
        
        # Stockage en base de donn√©es
        logger.info("[STORE] PHASE 2: Stockage des donn√©es en base...")
        logger.info(f"Donn√©es √† stocker: {len(questions_data)} questions")
        
        storage_start = datetime.now()
        await db_manager.store_questions(questions_data)
        storage_time = datetime.now() - storage_start
        logger.info(f"[OK] Stockage termin√© en {storage_time.total_seconds():.1f}s")
        
        # Analyse des donn√©es
        if analyze_data:
            logger.info("[ANALYZE] PHASE 3: Analyse des donn√©es...")
            logger.info("Initialisation de l'analyseur...")
            
            analyzer = DataAnalyzer(db_manager)
            analysis_start = datetime.now()
            logger.info("D√©marrage de l'analyse des tendances...")
            
            analysis_results = await analyzer.analyze_trends()
            analysis_time = datetime.now() - analysis_start
            logger.info(f"[OK] Analyse termin√©e en {analysis_time.total_seconds():.1f}s")
            
            # Sauvegarde des r√©sultats d'analyse
            logger.info("[STORE] Sauvegarde des r√©sultats d'analyse...")
            save_start = datetime.now()
            await analyzer.save_results(analysis_results)
            
            # G√©n√©ration des visualisations
            logger.info("[VIZ] G√©n√©ration des visualisations...")
            await analyzer.generate_visualizations(analysis_results)
            
            save_time = datetime.now() - save_start
            logger.info(f"[OK] Sauvegarde et visualisations termin√©es en {save_time.total_seconds():.1f}s")
        else:
            logger.info("‚è≠Ô∏è  Analyse des donn√©es d√©sactiv√©e")
        
        total_time = datetime.now() - start_time
        logger.info("üéâ PROCESSUS TERMIN√â AVEC SUCC√àS!")
        logger.info(f"Temps total:  Temps total d'ex√©cution: {total_time.total_seconds():.1f}s")
        logger.info(f"Questions extraites: R√©sum√©: {len(questions_data)} questions trait√©es")
        
        # Fermeture des connexions
        logger.info("üîå Fermeture des connexions...")
        await db_manager.disconnect()
        await scraper.close()
        logger.info("[OK] Toutes les connexions ferm√©es")
        
    except Exception as e:
        logger.error(f"Erreur lors de l'ex√©cution : {e}")
        # Nettoyage en cas d'erreur
        try:
            if 'db_manager' in locals():
                await db_manager.disconnect()
            if 'scraper' in locals():
                await scraper.close()
        except:
            pass
        raise


def parse_arguments():
    """Parse les arguments de ligne de commande."""
    parser = argparse.ArgumentParser(
        description="Stack Overflow Data Scraper and Analyzer"
    )
    
    parser.add_argument(
        "--max-questions", "-n",
        type=int,
        default=300,
        help="Nombre maximum de questions √† extraire (d√©faut: 300)"
    )
    
    parser.add_argument(
        "--tags", "-t",
        nargs="+",
        help="Tags √† filtrer (ex: python javascript react)"
    )
    
    parser.add_argument(
        "--use-api",
        action="store_true",
        help="Utiliser l'API Stack Overflow au lieu du scraping"
    )
    
    parser.add_argument(
        "--no-analysis",
        action="store_true",
        help="D√©sactiver l'analyse des donn√©es"
    )
    
    parser.add_argument(
        "--log-level",
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Niveau de logging (d√©faut: INFO)"
    )
    
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_arguments()
    setup_logging(args.log_level)
    
    # Ex√©cution asynchrone du scraper
    asyncio.run(main(
        max_questions=args.max_questions,
        tags=args.tags,
        use_api=args.use_api,
        analyze_data=not args.no_analysis
    ))
