#!/usr/bin/env python3
"""
Script de lancement des tests avec logging
=========================================

Script pour exÃ©cuter la suite de tests avec un logging complet
et gÃ©nÃ©rer automatiquement un rapport des rÃ©sultats.
"""

import subprocess
import sys
import os
import time
from pathlib import Path
from datetime import datetime


def run_tests_with_logging():
    """ExÃ©cute les tests avec logging complet."""
    
    print("ğŸ”¬ LANCEMENT DE LA SUITE DE TESTS AVEC LOGGING")
    print("=" * 60)
    
    # S'assurer que nous sommes dans le bon rÃ©pertoire
    script_dir = Path(__file__).parent
    os.chdir(script_dir)
    
    print(f"ğŸ“ RÃ©pertoire de travail: {script_dir}")
    print(f"â° DÃ©but d'exÃ©cution: {time.strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Commande pytest avec toutes les options de logging
    cmd = [
        sys.executable, "-m", "pytest",
        "tests/",
        "-v",
        "--tb=short"
    ]
    
    print("ğŸš€ Commande d'exÃ©cution:")
    print(f"   {' '.join(cmd)}")
    print()
    
    # ExÃ©cuter les tests
    start_time = time.time()
    
    try:
        result = subprocess.run(cmd, capture_output=False, text=True)
        exit_code = result.returncode
    except KeyboardInterrupt:
        print("\nâš ï¸  Tests interrompus par l'utilisateur")
        exit_code = 130
    except Exception as e:
        print(f"\nâŒ Erreur lors de l'exÃ©cution des tests: {e}")
        exit_code = 1
    
    duration = time.time() - start_time
    
    print()
    print("=" * 60)
    print("ğŸ“Š RÃ‰SULTATS DE L'EXÃ‰CUTION")
    print("=" * 60)
    print(f"â±ï¸  DurÃ©e totale: {duration:.2f} secondes")
    print(f"ğŸ”š Code de sortie: {exit_code}")
    
    # InterprÃ©ter le code de sortie
    if exit_code == 0:
        print("âœ… Tous les tests ont rÃ©ussi!")
    elif exit_code == 1:
        print("âŒ Certains tests ont Ã©chouÃ©")
    elif exit_code == 2:
        print("âš ï¸  Erreur d'exÃ©cution ou d'usage de pytest")
    elif exit_code == 3:
        print("ğŸ›‘ Tests interrompus par l'utilisateur")
    elif exit_code == 4:
        print("âš ï¸  Erreur interne de pytest")
    elif exit_code == 5:
        print("ğŸ” Aucun test trouvÃ©")
    else:
        print(f"âš ï¸  Code de sortie inattendu: {exit_code}")
    
    print()
    
    # VÃ©rifier si les fichiers de log ont Ã©tÃ© crÃ©Ã©s
    logs_dir = Path("tests/logs")
    if logs_dir.exists():
        log_files = list(logs_dir.glob("*.log"))
        if log_files:
            print("ğŸ“„ Fichiers de log gÃ©nÃ©rÃ©s:")
            for log_file in sorted(log_files, key=os.path.getmtime, reverse=True)[:3]:
                size = log_file.stat().st_size
                size_str = f"{size} bytes" if size < 1024 else f"{size/1024:.1f} KB"
                print(f"   â€¢ {log_file.name} ({size_str})")
    
    print()
    print("ğŸ’¡ Pour analyser les rÃ©sultats dÃ©taillÃ©s:")
    print("   python tests/analyze_logs.py --show")
    print()
    
    return exit_code


def generate_test_report():
    """GÃ©nÃ¨re automatiquement un rapport de tests et le sauvegarde dans output/reports."""
    
    print("ğŸ“Š GÃ‰NÃ‰RATION DU RAPPORT DE TESTS")
    print("=" * 40)
    
    try:
        # Importer l'analyseur de logs
        sys.path.append(str(Path(__file__).parent / "tests"))
        from analyze_logs import TestLogAnalyzer
        
        # Initialiser l'analyseur
        analyzer = TestLogAnalyzer()
        
        # RÃ©cupÃ©rer les derniers logs
        log_files = analyzer.get_latest_logs()
        
        if not log_files:
            print("âŒ Aucun fichier de log trouvÃ© pour gÃ©nÃ©rer le rapport!")
            return False
            
        print("ğŸ“ Analyse des fichiers de log:")
        for log_type, path in log_files.items():
            print(f"  â€¢ {log_type}: {path.name}")
        
        # Analyser les rÃ©sultats
        main_log = log_files.get('run')
        if not main_log:
            print("âŒ Fichier de log principal non trouvÃ©!")
            return False
            
        results = analyzer.parse_test_results(main_log)
        
        # GÃ©nÃ©rer le rapport
        report = analyzer.generate_report(results)
        
        # CrÃ©er le rÃ©pertoire output/reports s'il n'existe pas
        reports_dir = Path("output/reports")
        reports_dir.mkdir(parents=True, exist_ok=True)
        
        # Nom du fichier de rapport avec timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_filename = f"rapport_tests_{timestamp}.md"
        report_path = reports_dir / report_filename
        
        # Convertir le rapport en format Markdown
        markdown_report = convert_to_markdown(report, results)
        
        # Sauvegarder le rapport
        with open(report_path, 'w', encoding='utf-8') as f:
            f.write(markdown_report)
        
        print(f"âœ… Rapport gÃ©nÃ©rÃ© avec succÃ¨s: {report_path}")
        print(f"ğŸ“„ Taille du rapport: {report_path.stat().st_size} bytes")
        
        return True
        
    except Exception as e:
        print(f"âŒ Erreur lors de la gÃ©nÃ©ration du rapport: {e}")
        return False


def convert_to_markdown(report: str, results: dict) -> str:
    """Convertit le rapport texte en format Markdown."""
    
    timestamp = datetime.now().strftime("%Y-%m-%d Ã  %H:%M:%S")
    total_tests = len(results['passed']) + len(results['failed']) + len(results.get('errors', [])) + len(results['skipped'])
    success_rate = len(results['passed']) / total_tests * 100 if total_tests > 0 else 0
    
    markdown = f"""# ğŸ§ª RAPPORT DE TESTS - STACK OVERFLOW SCRAPER

*Rapport d'exÃ©cution de la suite de tests*

---

## ğŸ“Š RÃ‰SUMÃ‰ EXÃ‰CUTIF

### Informations gÃ©nÃ©rales

- **Date d'exÃ©cution**: {timestamp}
- **Tests totaux exÃ©cutÃ©s**: {total_tests}
- **DurÃ©e totale**: {results['total_duration']:.2f} secondes
- **Taux de rÃ©ussite**: {success_rate:.1f}%

### RÃ©sultats par catÃ©gorie

| Statut | Nombre | Pourcentage |
|--------|--------|-------------|
| âœ… **RÃ©ussis** | {len(results['passed'])} | {len(results['passed'])/total_tests*100:.1f}% |
| âŒ **Ã‰chouÃ©s** | {len(results['failed'])} | {len(results['failed'])/total_tests*100:.1f}% |
| ğŸš« **Erreurs** | {len(results.get('errors', []))} | {len(results.get('errors', []))/total_tests*100:.1f}% |
| â­ï¸ **IgnorÃ©s** | {len(results['skipped'])} | {len(results['skipped'])/total_tests*100:.1f}% |

## ğŸ“ˆ ANALYSE DÃ‰TAILLÃ‰E

"""

    # Ajouter les tests rÃ©ussis
    if results['passed']:
        markdown += f"""### âœ… Tests RÃ©ussis ({len(results['passed'])})

| Test | DurÃ©e |
|------|-------|
"""
        for test in results['passed']:
            markdown += f"| `{test['name']}` | {test['duration']:.2f}s |\n"
        markdown += "\n"

    # Ajouter les erreurs de tests
    if results.get('errors'):
        markdown += f"""### ğŸš« Erreurs de Tests ({len(results['errors'])})

| Test | DurÃ©e | DÃ©tails |
|------|-------|---------|
"""
        for test in results['errors']:
            markdown += f"| `{test['name']}` | {test['duration']:.2f}s | âš ï¸ Erreur de configuration/setup |\n"
        markdown += "\n"

    # Ajouter les tests Ã©chouÃ©s
    if results['failed']:
        markdown += f"""### âŒ Tests Ã‰chouÃ©s ({len(results['failed'])})

| Test | DurÃ©e | DÃ©tails |
|------|-------|---------|
"""
        for test in results['failed']:
            markdown += f"| `{test['name']}` | {test['duration']:.2f}s | âš ï¸ Assertion Ã©chouÃ©e |\n"
        markdown += "\n"

    # Ajouter les tests ignorÃ©s
    if results['skipped']:
        markdown += f"""### â­ï¸ Tests IgnorÃ©s ({len(results['skipped'])})

| Test | Raison |
|------|--------|
"""
        for test in results['skipped']:
            markdown += f"| `{test['name']}` | IgnorÃ© |\n"
        markdown += "\n"

    # Tests les plus lents
    if results['passed'] or results['failed'] or results.get('errors'):
        all_timed_tests = results['passed'] + results['failed'] + results.get('errors', [])
        slowest = sorted(all_timed_tests, key=lambda x: x['duration'], reverse=True)[:5]
        
        markdown += f"""### ğŸŒ Tests les Plus Lents

| Rang | Test | DurÃ©e | Statut |
|------|------|-------|--------|
"""
        for i, test in enumerate(slowest, 1):
            if test in results['passed']:
                status = "âœ… RÃ©ussi"
            elif test in results['failed']:
                status = "âŒ Ã‰chouÃ©"
            else:
                status = "ğŸš« Erreur"
            markdown += f"| {i} | `{test['name']}` | {test['duration']:.2f}s | {status} |\n"
        markdown += "\n"

    # Recommandations
    markdown += """## ğŸ’¡ RECOMMANDATIONS

"""
    
    if results.get('errors'):
        markdown += "- ğŸš« **PrioritÃ© critique**: Corriger les erreurs de configuration/setup listÃ©es ci-dessus\n"
    
    if results['failed']:
        markdown += "- ğŸ”§ **PrioritÃ© haute**: Corriger les tests Ã©chouÃ©s (assertions) listÃ©s ci-dessus\n"
    
    if len(results['skipped']) > len(results['passed']) / 4:
        markdown += "- ğŸ” **Analyse requise**: Beaucoup de tests ignorÃ©s - vÃ©rifier s'ils peuvent Ãªtre activÃ©s\n"
    
    if results['total_duration'] > 60:
        markdown += "- âš¡ **Optimisation**: DurÃ©e d'exÃ©cution Ã©levÃ©e - optimiser les tests les plus lents\n"
    
    if not results['failed'] and not results.get('errors') and not results['skipped']:
        markdown += "- ğŸ‰ **Excellent**: Tous les tests passent avec succÃ¨s!\n"
    elif not results['failed'] and not results.get('errors'):
        markdown += "- ğŸ‰ **TrÃ¨s bon**: Tous les tests actifs passent avec succÃ¨s!\n"

    markdown += f"""
## ğŸ“‹ DÃ‰TAILS TECHNIQUES

### Configuration de test

- **Framework**: pytest
- **RÃ©pertoire de tests**: `tests/`
- **Mode d'exÃ©cution**: Verbeux avec traces courtes
- **Logs gÃ©nÃ©rÃ©s**: `tests/logs/`

### Performance

- **Vitesse moyenne**: {total_tests/results['total_duration']:.1f} tests/seconde
- **Test le plus rapide**: {min([t['duration'] for t in results['passed'] + results['failed']] or [0]):.3f}s
- **Test le plus lent**: {max([t['duration'] for t in results['passed'] + results['failed']] or [0]):.3f}s

---
**Rapport gÃ©nÃ©rÃ© le {timestamp}**  
*Stack Overflow Scraper - Suite de tests automatisÃ©e*
"""

    return markdown


if __name__ == "__main__":
    exit_code = run_tests_with_logging()
    
    # GÃ©nÃ©rer automatiquement le rapport de tests
    print()
    print("ğŸ”„ GÃ‰NÃ‰RATION AUTOMATIQUE DU RAPPORT")
    print("=" * 40)
    
    if generate_test_report():
        print("âœ… Rapport de tests gÃ©nÃ©rÃ© avec succÃ¨s!")
    else:
        print("âš ï¸ Impossible de gÃ©nÃ©rer le rapport de tests")
    
    print()
    sys.exit(exit_code)
